test in main: 3===>OK 1

test in main: {"id": "3", "uuid": "ebc82dade09135a0fd0c4ccbc5844aa2", "body": "\"VizSeq: A Visual Analysis Toolkit for Text Generation TasksChanghan Wang\\u2020, Anirudh Jain*\\u2021, Danlu Chen\\u2020 and Jiatao Gu\\u2020\\n\\u2020 Facebook AI Research, \\u2021 Stanford University{changhan, danluchen, jgu}@fb.com, anirudhj@stanford.eduAbstractAutomatic evaluation of text generation tasks\\n(e.g. machine translation, text summariza-\\ntion, image captioning and video description)\\nusually relies heavily on task-specific metrics,\\nsuch as BLEU (Papineni et al., 2002) and\\nROUGE (Lin, 2004). They, however, are ab-\\nstract numbers and are not perfectly aligned\\nwith human assessment. This suggests in-\\nspecting detailed examples as a complement\\nto identify system error patterns. In this paper,\\nwe present VizSeq, a visual analysis toolkit for\\ninstance-level and corpus-level system eval-\\nuation on a wide variety of text generation\\ntasks. It supports multimodal sources and mul-\\ntiple text references, providing visualization in\\nJupyter notebook or a web app interface. It can\\nbe used locally or deployed onto public servers\\nfor centralized data hosting and benchmark-\\ning. It covers most common n-gram based\\nmetrics accelerated with multiprocessing, and\\nalso provides latest embedding-based metrics\\nsuch as BERTScore (Zhang et al., 2019).1 IntroductionMany natural language processing (NLP) tasks\\ncan be viewed as conditional text generation prob-\\nlems, where natural language texts are generated\\ngiven inputs in the form of text (e.g. machine\\ntranslation), image (e.g. image captioning), au-\\ndio (e.g. automatic speech recognition) or video\\n(e.g. video description). Their automatic evalu-\\nation usually relies heavily on task-specific met-\\nrics. Due to the complexity of natural language\\nexpressions, those metrics are not always perfectly\\naligned with human assessment. Moreover, met-\\nrics only produce abstract numbers and are limited\\nin illustrating system error patterns. This suggests\\nthe necessity of inspecting detailed evaluation ex-\\namples to get a full picture of system behaviors as* Work carried out during an internship at Facebook.Sources\\nTextImage\\nAudioReferences\\nText 1\\nText 2Text N\\u2026Predictions\\nModel 1 Text\\nModel 2 TextModel K Text\\u2026VizSeqWeb AppJupyter NotebookVideoevaluate.pymulti-process\\nscorersFigure 1: An overview of VizSeq. VizSeq takes multi-\\nmodal sources, text references as well as model predic-\\ntions as inputs, and analyzes them visually in Jupyter\\nnotebook or in a web app interface. It can also be used\\nwithout visualization as a normal Python package.well as seek improvement directions.\\nA bunch of softwares have emerged to facili-tate calculation of various metrics or demonstrat-\\ning examples with sentence-level scores in an in-\\ntegrated user interface: ibleu (Madnani, 2011),\\nMTEval 1, MT-ComparEval (Klejch et al., 2015),\\nnlg-eval (Sharma et al., 2017), Vis-Eval Met-\\nric Viewer (Steele and Specia, 2018), compare-\\nmt (Neubig et al., 2019), etc. Quite a few of them\\nare collections of command-line scripts for met-\\nric calculation, which lack visualization to bet-\\nter present and interpret the scores. Some of\\nthem are able to generate static HTML reports to\\npresent charts and examples, but they do not al-\\nlow updating visualization options interactively.\\nMT-ComparEval is the only software we found\\nthat has an interactive user interface. It is, how-\\never, written in PHP, which unlike Python lacks a\\ncomplete NLP eco-system. The number of met-\\nrics it supports is also limited and the software\\nis no longer being actively developed. Support\\nof multiple references is not a prevalent stan-\\ndard across all the softwares we investigated, and1https://github.com/odashi/mtevalar\\nXiv\\n:190\\n9.05\\n424v\\n1  [\\ncs.C\\nL] \\n 12 \\nSep\\n 201\\n9Source Type Example TasksText machine translation, text summarization,\\ndialog generation, grammatical error cor-\\nrection, open-domain question answeringImage image captioning, visual question answer-\\ning, optical character recognitionAudio speech recognition, speech translation\\nVideo video description\\nMultimodal multimodal machine translationTable 1: Example text generation tasks supported by\\nVizSeq. The sources can be from various modalities.Metrics VizSeq compare-\\nmtnlg-\\nevalMT-\\nCompar-\\nEvalBLEU\\nchrF\\nMETEOR\\nTER\\nRIBES\\nGLEU\\nNIST\\nROUGE\\nCIDEr\\nWERLASER\\nBERTScoreTable 2: Comparison of VizSeq and its counterparts on\\nn-gram-based and embedding-based metric coverage.none of them supports multiple sources or sources\\nin non-text modalities such as image, audio and\\nvideo. Almost all the metric implementations are\\nsingle-processed, which cannot leverage the mul-\\ntiple cores in modern CPUs for speedup and better\\nscalability.With the above limitations identified, we want\\nto provide a unified and scalable solution that\\ngets rid of all those constraints and is enhanced\\nwith a user-friendly interface as well as the lat-\\nest NLP technologies. In this paper, we present\\nVizSeq, a visual analysis toolkit for a wide va-\\nriety of text generation tasks, which can be used\\nfor: 1) instance-level and corpus-level system er-\\nror analysis; 2) exploratory dataset analysis; 3)\\npublic data hosting and system benchmarking. It\\nprovides visualization in Jupyter notebook or a\\nweb app interface. A system overview can be\\nfound in Figure 1. We open source the software\\nat https://github.com/facebookresearch/vizseq.BLEU METEOR chrF CIDEr\\n0\\n510\\n15\\n20\\n25\\n30\\n35T\\nime\\n(sec\\nonds\\n) 1 process2 processes\\n4 processesFigure 2: VizSeq implements metrics with multipro-\\ncessing speedup. Speed test is based on a 36k evalua-\\ntion set for BLEU, METEOR and chrF, and a 41k one\\nfor CIDEr. CPU: Intel Core i7-7920HQ @ 3.10GHz2 Main Features of VizSeq2.1 Multimodal Data and Task CoverageVizSeq has built-in support for multiple sources\\nand references. The number of references is al-\\nlowed to vary across different examples, and the\\nsources are allowed to come from different modal-\\nities, including text, image, audio and video. This\\nflexibility enables VizSeq to cover a wide range of\\ntext generation tasks and datasets, far beyond the\\nscope of machine translation, which previous soft-\\nwares mainly focus on. Table 1 provides a list of\\nexample tasks supported by Vizseq.2.2 Metric Coverage and ScalabilityTable 2 shows the comparison of VizSeq and its\\ncounterparts on metric coverage.N-gram-based metrics To the extent of our\\nknowledge, VizSeq has the best coverage of com-\\nmon n-gram-based metrics, including BLEU (Pap-\\nineni et al., 2002), NIST (Doddington, 2002), ME-\\nTEOR (Banerjee and Lavie, 2005), TER (Snover\\net al., 2006), RIBES (Isozaki et al., 2010),\\nchrF (Popovic\\u0301, 2015) and GLEU (Wu et al.,\\n2016) for machine translation; ROUGE (Lin,\\n2004) for summarization and video description;\\nCIDEr (Vedantam et al., 2015) for image caption-\\ning; and word error rate for speech recognition.Embedding-based metrics N-gram-based met-\\nrics have difficulties in capturing semantic sim-\\nilarities since they are usually based on ex-\\nact word matches. As a complement, VizSeq\\nalso integrates latest embedding-based metrics\\nsuch as BERTScore (Zhang et al., 2019) and\\nLASER (Artetxe and Schwenk, 2018). This is\\nrarely seen in the counterparts.Scalability We re-implemented all the n-gram-\\nbased metrics with multiprocessing, allowinghttps://github.com/facebookresearch/vizseq\\n1 from vizseq.scorers import\\nregister_scorer23 @register_scorer(\\u2019metric name\\u2019)\\n4 def calculate_score(\\n5 hypothesis: List[str],\\n6 references: List[List[str]],\\n7 n_processes: int = 2,\\n8 verbose: bool = False\\n9 ) -> Tuple[float, List[float]]:10 return corpus_score, sentence_scoresFigure 3: VizSeq metric API. Users can define and reg-\\nister their new metric by implementing this function.users to fully utilize the power of modern multi-\\ncore CPUs. We tested our multi-process versions\\non large evaluation sets and observed significant\\nspeedup against original single-process ones (see\\nFigure 2). VizSeq\\u2019s embedding-based metrics are\\nimplemented using PyTorch (Paszke et al., 2017)\\nframework and their computation is automatically\\nparallelized on CPU or GPU by the framework.Versatility VizSeq\\u2019s rich metric collection is not\\nonly available in Jupyter notebook or in the web\\napp, it can also be used in any Python scripts.\\nA typical use case is periodic metric calculation\\nduring model training. VizSeq\\u2019s implementations\\nsave time, especially when evaluation sets are\\nlarge or evaluation is frequent. To allow user-\\ndefined metrics, we designed an open metric API,\\nwhose definition can be found in Figure 3.2.3 User-Friendly InterfaceGiven the drawbacks of simple command-line in-\\nterface and static HTML interface, we aim at visu-\\nalized and interactive interfaces for better user ex-\\nperience and productivity. VizSeq provides visu-\\nalization in two types of interfaces: Jupyter note-\\nbook and web app. They share the same visual\\nanalysis module (Figure 4). The web app interface\\nadditionally has a data uploading module (Fig-\\nure 9) and a task/dataset browsing module (Fig-\\nure 10), while the Jupyter notebook interface gets\\ndata directly from Python variables. The analysis\\nmodule includes the following parts.Example grouping VizSeq uses sentence tags\\nto manage example groups (data subsets of differ-\\nent interest, can be overlapping). It contains both\\nuser-defined and machine-generated tags (e.g. la-\\nbels for identified languages, long sentences, sen-\\ntences with rare words or code-switching). Met-\\nrics will be calculated and visualized by different1       2       3       4       5       Figure 4: VizSeq example viewing. (1) keyword search\\nbox, tag and model filters, sorting and page size op-\\ntions; (2) left: example index, right: user-defined tags\\n(blue) and machine-generated tags (grey); (3) multi-\\nmodal sources and Google Translate integration; (4)\\nmodel predictions with highlighted matched (blue) and\\nunmatched (red) n-grams; (5) sentence-level scores\\n(highest ones among models in boldface, lowest ones\\nin italics with underscore).example groups as a complement to scores over\\nthe entire dataset.Example viewing VizSeq presents examples\\nwith various sentence-level scores and visualized\\nalignments of matched/unmatched reference n-\\ngrams in model predictions. It also has Google\\nTranslate integration to assist understanding of\\ntext sources in unfamiliar languages as well as\\nproviding a baseline translation model. Exam-\\nples are listed in multiple pages (bookmarkable in\\nweb app) and can be sorted by various orders, for\\nexample, by a certain metric or source sentence\\nlengths. Tags or n-gram keywords can be used to\\nfilter out examples of interest.Dataset statistics VizSeq provides various\\ncorpus-level statistics, including: 1) counts of\\nsentences, tokens and characters; 2) source and\\nreference length distributions; 3) token frequency\\ndistribution; 4) list of most frequent n-grams (with\\nlinks to associated examples); 5) distributions\\nof sentence-level scores by models (Figure 5, 6\\nand 7). Statistics are visualized in zoomable\\ncharts with hover text hints.Data export Statistics in VizSeq are one-click\\nexportable: charts into PNG or SVG images (with12 3Figure 5: VizSeq dataset statistics. (1) sentence, to-\\nken and character counts for source and reference sen-\\ntences; (2) length distributions of source and reference\\nsentences; (3) token frequency distribution. Plots are\\nzoomable and exportable to SVG or PNG images.Figure 6: VizSeq dataset statistics: most frequent n-\\ngrams (n=1,2,3,4). Each listed n-gram is clickable to\\nshow associated examples in the dataset.users\\u2019 zooming applied) and tables into CSV or\\nLATEX (copied to clipboard).2.4 Data Management and Public HostingVizSeq web app interface gets new data from the\\ndata uploading module (Figure 9) or a REST-\\nful API. Besides local deployment, the web app\\nback-end can also be deployed onto public servers\\nand provide a general solution for hosting public\\nbenchmarks on a wide variety of text generation\\ntasks and datasets.In VizSeq, data is organized by special folder\\nstructures as follows, which is easy to maintain:<task>/<eval set>/source_*.{txt,zip}\\n<task>/<eval set>/reference_*.txt\\n<task>/<eval set>/tag_*.txt\\n<task>/<eval set>/<model>/prediction.txt\\n<task>/<eval set>/__cfg__.jsonWhen new data comes in, scores, n-grams and\\nmachine-generated tags will be pre-computed and\\ncached onto disk automatically. A file monitoring\\nand versioning system (based on file hashes, sizes\\nor modification timestamps) is employed to detect12 3Figure 7: VizSeq corpus-level metric viewing. (1) dis-\\ntributions of sentence-level scores by models; (2) one-\\nclick export of tabular data to CSV and LATEX (copied\\nto clipboard); (3) corpus-level and group-level (by sen-\\ntence tags) scores (highest ones among models in bold-\\nface, lowest ones in italics with underscore).Figure 8: VizSeq sentence tag distribution view. In this\\nexample, tags are source-target language directions in\\na multilingual machine translation dataset.file changes and trigger necessary updates on pre-\\ncomputed results. This is important for support-\\ning evaluation during model training where model\\npredictions change over training epochs.3 Example Use Cases of VizSeqWe validate the usability of VizSeq with multiple\\ntasks and datasets, which are included as examples\\nin our Github repository:\\u2022 WMT14 English-German2: a classic medium-\\nsize dataset for bilingual machine translation.\\u2022 Gigaword3: a text summarization dataset.\\u2022 COCO captioning 2015 (Lin et al., 2014): a\\nclassic image captioning dataset where VizSeq\\ncan present source images with text targets.2http://www.statmt.org/wmt14/translation-task.html\\n3https://github.com/harvardnlp/sent-summaryhttp://www.statmt.org/wmt14/translation-task.html\\nhttps://github.com/harvardnlp/sent-summary\\nFigure 9: VizSeq data uploading. Users need to orga-\\nnize the files by given folder structures and pack them\\ninto a zip file for upload. VizSeq will unpack the files\\nto the data root folder and perform integrity checks.Figure 10: VizSeq task/dataset browsing. Users need\\nto select a dataset and models of interest to proceed to\\nthe analysis module.\\u2022 WMT16 multimodal machine translation task\\n14: English-German translation with an image\\nthe sentences describe. VizSeq can present both\\ntext and image sources, and calculate the official\\nBLEU, METEOR and TER metrics.\\u2022 Multilingual machine translation on TED talks\\ndataset (Ye et al., 2018): translation from 58\\nlanguages into English. VizSeq can use lan-\\nguage directions as sentence tags to generate\\nscore breakdown by languages. The test set has\\nas many as 165k examples, where VizSeq multi-\\nprocess scorers run significantly faster than\\nsingle-process ones. The integrated Google\\nTranslate can help with understanding source\\nsentences in unfamiliar languages.\\u2022 IWSLT17 English-German speech translation5:\\nVizSeq can present English audios with English\\ntranscripts and German text translations.\\u2022 YouCook (Das et al., 2013) video description:\\nVizSeq enables inspecting generated text de-\\n4https://www.statmt.org/wmt16/multimodal-task.html\\n5https://sites.google.com/site/iwsltevaluation2017scriptions with presence of video contents.4 Related WorkWith the thrive of deep learning, task-agnostic\\nvisualization toolkits such as Tensorboard6, vis-\\ndom7 and TensorWatch8, have emerged in need of\\nmonitoring model statistics and debugging model\\ntraining. Model interpretability is another moti-\\nvation for visualization. In NLP, softwares have\\nbeen developed to interpret model parameters (e.g.\\nattention weights) and inspect prediction genera-\\ntion process: LM (Rong and Adar, 2016), Open-\\nNMT visualization tool (Klein et al., 2018) and\\nSeq2Seq (Strobelt et al., 2019). For machine trans-\\nlation, toolkits are made to perform metric calcu-\\nlation and error analysis: ibleu (Madnani, 2011),\\nMTEval 9, MT-ComparEval (Klejch et al., 2015),\\nnlg-eval (Sharma et al., 2017), Vis-Eval Metric\\nViewer (Steele and Specia, 2018) and compare-\\nmt (Neubig et al., 2019).5 ConclusionIn this paper, we present VizSeq, a visual anal-\\nysis toolkit for {text, image, audio, video}-to-\\ntext generation system evaluation, dataset analysis\\nand benchmark hosting. It is accessible as a web\\napp or a Python package in Jupyter notebook or\\nPython scripts. VizSeq is currently under active\\ndevelopment and our future work includes: 1) en-\\nabling image-to-text and video-to-text alignments;\\n2) adding human assessment modules; 3) integra-\\ntion with popular text generation frameworks such\\nas fairseq10, opennmt11 and tensor2tensor12.AcknowledgmentsWe thank the anonymous reviewers for their com-\\nments. We also thank Ann Lee and Pratik Ring-\\nshia for helpful discussions on this project.References\\nMikel Artetxe and Holger Schwenk. 2018. Mas-sively multilingual sentence embeddings for zero-\\nshot cross-lingual transfer and beyond. arXiv\\npreprint arXiv:1812.10464.6https://github.com/tensorflow/tensorboard\\n7https://github.com/facebookresearch/visdom\\n8https://github.com/microsoft/tensorwatch\\n9https://github.com/odashi/mteval10https://github.com/pytorch/fairseq\\n11https://github.com/OpenNMT/OpenNMT-py\\n12https://github.com/tensorflow/tensor2tensorhttps://www.statmt.org/wmt16/multimodal-task.html\\nhttps://sites.google.com/site/iwsltevaluation2017\\nhttps://github.com/tensorflow/tensorboard\\nhttps://github.com/facebookresearch/visdom\\nhttps://github.com/microsoft/tensorwatch\\nhttps://github.com/odashi/mteval\\nhttps://github.com/pytorch/fairseq\\nhttps://github.com/OpenNMT/OpenNMT-py\\nhttps://github.com/tensorflow/tensor2tensor\\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\\nautomatic metric for mt evaluation with improved\\ncorrelation with human judgments. In Proceedings\\nof the acl workshop on intrinsic and extrinsic evalu-\\nation measures for machine translation and/or sum-\\nmarization, pages 65\\u201372.Pradipto Das, Chenliang Xu, Richard F Doell, and Ja-\\nson J Corso. 2013. A thousand frames in just a few\\nwords: Lingual description of videos through latent\\ntopics and sparse object stitching. In Proceedings of\\nthe IEEE conference on computer vision and pattern\\nrecognition, pages 2634\\u20132641.George Doddington. 2002. Automatic evaluation\\nof machine translation quality using n-gram co-\\noccurrence statistics. In Proceedings of the second\\ninternational conference on Human Language Tech-\\nnology Research, pages 138\\u2013145. Morgan Kauf-\\nmann Publishers Inc.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito\\nSudoh, and Hajime Tsukada. 2010. Automatic eval-\\nuation of translation quality for distant language\\npairs. In Proceedings of the 2010 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, pages 944\\u2013952. Association for Computational\\nLinguistics.Guillaume Klein, Yoon Kim, Yuntian Deng, Vincent\\nNguyen, Jean Senellart, and Alexander M. Rush.\\n2018. Opennmt: Neural machine translation toolkit.Ondr\\u030cej Klejch, Eleftherios Avramidis, Aljoscha Bur-\\nchardt, and Martin Popel. 2015. Mt-compareval:\\nGraphical evaluation interface for machine transla-\\ntion development. The Prague Bulletin of Mathe-\\nmatical Linguistics, 104(1):63\\u201374.Chin-Yew Lin. 2004. Rouge: A package for auto-\\nmatic evaluation of summaries. Text Summarization\\nBranches Out.Tsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Dolla\\u0301r,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision, pages 740\\u2013755. Springer.Nitin Madnani. 2011. ibleu: Interactively debugging\\nand scoring statistical machine translation systems.\\nIn 2011 IEEE Fifth International Conference on Se-\\nmantic Computing, pages 213\\u2013214. IEEE.Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel,\\nDanish Pruthi, and Xinyi Wang. 2019. compare-mt:\\nA tool for holistic comparison of language genera-\\ntion systems. arXiv preprint arXiv:1903.07926.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic eval-\\nuation of machine translation. In Proceedings of\\nthe 40th annual meeting on association for compu-\\ntational linguistics, pages 311\\u2013318. Association for\\nComputational Linguistics.Adam Paszke, Sam Gross, Soumith Chintala, Gre-\\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\\ning Lin, Alban Desmaison, Luca Antiga, and Adam\\nLerer. 2017. Automatic differentiation in PyTorch.\\nIn NIPS Autodiff Workshop.Maja Popovic\\u0301. 2015. chrf: character n-gram f-score\\nfor automatic mt evaluation. In Proceedings of the\\nTenth Workshop on Statistical Machine Translation,\\npages 392\\u2013395.Xin Rong and Eytan Adar. 2016. Visual tools for de-\\nbugging neural language models. In Proceedings of\\nICML Workshop on Visualization for Deep Learn-\\ning.Shikhar Sharma, Layla El Asri, Hannes Schulz, and\\nJeremie Zumer. 2017. Relevance of unsupervised\\nmetrics in task-oriented dialogue for evaluating nat-\\nural language generation. CoRR, abs/1706.09799.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\\nnea Micciulla, and John Makhoul. 2006. A study of\\ntranslation edit rate with targeted human annotation.\\nIn Proceedings of association for machine transla-\\ntion in the Americas, volume 200.David Steele and Lucia Specia. 2018. Vis-eval metric\\nviewer: A visualisation tool for inspecting and eval-\\nuating metric scores of machine translation output.\\nIn Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Demonstrations, pages 71\\u201375.Hendrik Strobelt, Sebastian Gehrmann, Michael\\nBehrisch, Adam Perer, Hanspeter Pfister, and\\nAlexander M Rush. 2019. Seq2seq-vis: A visual\\ndebugging tool for sequence-to-sequence models.\\nIEEE transactions on visualization and computer\\ngraphics, 25(1):353\\u2013363.Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\\nParikh. 2015. Cider: Consensus-based image de-\\nscription evaluation. In Proceedings of the IEEE\\nconference on computer vision and pattern recog-\\nnition, pages 4566\\u20134575.Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google\\u2019s neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144.Qi Ye, Sachan Devendra, Felix Matthieu, Padmanab-\\nhan Sarguna, and Neubig Graham. 2018. When\\nand why are pre-trained word embeddings useful for\\nneural machine translation. In HLT-NAACL.Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\\nuating text generation with bert. arXiv preprint\\narXiv:1904.09675.http://arxiv.org/abs/1805.11462\\nhttp://arxiv.org/abs/1706.09799\\nhttp://arxiv.org/abs/1706.09799\\nhttp://arxiv.org/abs/1706.09799\"", "processed_body": "", "val": 0.01}===>OK 3

test in main: string indices must be integers===>Error DB

test in main: ===>OK Finally